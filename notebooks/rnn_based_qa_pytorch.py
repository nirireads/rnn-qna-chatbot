# -*- coding: utf-8 -*-
"""rnn-based-qa-pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C90B8qgAlEjIhuT3emR-cTmd7-U19GVm
"""

import pandas as pd

df = pd.read_csv('/content/100_Unique_QA_Dataset.csv')

print(df.head())

# tokanize - seperate each words
def tokenize(text):
  text = text.lower()
  text = text.replace('?', '')
  text = text.replace('"', '')
  text = text.replace(',', '')
  text = text.replace('!', '')
  text = text.replace('.', '')
  return text.split()

# vocabulary dictionary
vocab = {'<UNK>':0}

# function to build vocab
def build_vocab(row):
  # print(row['question'], row['answer'])
  tokenized_question =  tokenize(row['question'])
  tokenized_row = tokenize(row['answer'])
  # print(tokenized_question, tokenized_row)

  merged_tokens = tokenized_question + tokenized_row
  # print(merged_tokens)

  for token in merged_tokens:
    if token not in vocab:
      vocab[token] = len(vocab)
  # print(vocab)
  print(vocab)

# calling build_vocab for every row
df.apply(build_vocab, axis=1)

# words into numerical index
def text_to_indices(text,vocab):
  indexed_text = []

  for token in tokenize(text):
    if(token in vocab):
      indexed_text.append(vocab[token])
    else:
      indexed_text.append(vocab['<UNK>'])
  return indexed_text

text_to_indices("who is nir singh", vocab)

import torch
from torch.utils.data import Dataset, DataLoader

class QADataset(Dataset):
  def __init__(self, df, vocab):
   self.df = df
   self.vocab = vocab

  def __len__(self):
    return self.df.shape[0]

  def __getitem__(self, index):
    numerical_question = text_to_indices(self.df.iloc[index]['question'], self.vocab)
    numerical_answer = text_to_indices(self.df.iloc[index]['answer'], self.vocab)

    return torch.tensor(numerical_question), torch.tensor(numerical_answer)

dataset = QADataset(df, vocab)

dataset[0]

dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for question,answer in dataloader:
  print(question, answer)

# making RNN model
import torch.nn as nn

class SimpleRNN(nn.Module):
  def __init__(self, vocab_size):
    super().__init__()
    #learn and read about rnn in details
    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)
    self.rnn = nn.RNN(50, 64, batch_first=True)
    self.fc = nn.Linear(64, vocab_size)

  def forward(self, question):
    embedded_question = self.embedding(question)
    hidden, final = self.rnn(embedded_question)
    output = self.fc(final.squeeze(0))
    return output

learning_rate = 0.001
epochs = 20

type(epochs)

model = SimpleRNN(len(vocab))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#trainning  loop
for epoch in range(epochs):
  total_loss = 0

  for question, answer in dataloader:
    optimizer.zero_grad()
    output = model(question)
    loss = criterion(output, answer[0])
    loss.backward()
    optimizer.step()

    total_loss = total_loss + loss.item()

  print(f"Epoch: {epoch}, Loss: {total_loss}")

def predict(model, question, threshold=0.5):
  #convert question into numbers
  numerical_question = text_to_indices(question,vocab)
  print(numerical_question)

  #to tensor
  question_tensor = torch.tensor(numerical_question).unsqueeze(0)
  print(question_tensor.shape)

  #send to model
  output = model(question_tensor)
  print(output)
  print("output shape",output.shape)

  #convert logit to probs
  probs = torch.nn.functional.softmax(output, dim=1)
  print(probs)

  #find index of max probs
  value, index = torch.max(probs,dim=1)
  print(f"max value:{value} and index: {index}")

  #threshold
  if value<threshold:
    print("i dont know")
    answer = list(vocab.keys())[index]
    print("Answer =>", answer)
  else:
    answer = list(vocab.keys())[index]
    print("Answer =>", answer)

predict(model, "who write to kill mocking bird")

